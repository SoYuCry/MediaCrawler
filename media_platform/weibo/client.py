# å£°æ˜Žï¼šæœ¬ä»£ç ä»…ä¾›å­¦ä¹ å’Œç ”ç©¶ç›®çš„ä½¿ç”¨ã€‚ä½¿ç”¨è€…åº”éµå®ˆä»¥ä¸‹åŽŸåˆ™ï¼š  
# 1. ä¸å¾—ç”¨äºŽä»»ä½•å•†ä¸šç”¨é€”ã€‚  
# 2. ä½¿ç”¨æ—¶åº”éµå®ˆç›®æ ‡å¹³å°çš„ä½¿ç”¨æ¡æ¬¾å’Œrobots.txtè§„åˆ™ã€‚  
# 3. ä¸å¾—è¿›è¡Œå¤§è§„æ¨¡çˆ¬å–æˆ–å¯¹å¹³å°é€ æˆè¿è¥å¹²æ‰°ã€‚  
# 4. åº”åˆç†æŽ§åˆ¶è¯·æ±‚é¢‘çŽ‡ï¼Œé¿å…ç»™ç›®æ ‡å¹³å°å¸¦æ¥ä¸å¿…è¦çš„è´Ÿæ‹…ã€‚   
# 5. ä¸å¾—ç”¨äºŽä»»ä½•éžæ³•æˆ–ä¸å½“çš„ç”¨é€”ã€‚
#   
# è¯¦ç»†è®¸å¯æ¡æ¬¾è¯·å‚é˜…é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„LICENSEæ–‡ä»¶ã€‚  
# ä½¿ç”¨æœ¬ä»£ç å³è¡¨ç¤ºæ‚¨åŒæ„éµå®ˆä¸Šè¿°åŽŸåˆ™å’ŒLICENSEä¸­çš„æ‰€æœ‰æ¡æ¬¾ã€‚  


# -*- coding: utf-8 -*-
# @Author  : relakkes@gmail.com
# @Time    : 2023/12/23 15:40
# @Desc    : å¾®åšçˆ¬è™« API è¯·æ±‚ client

import asyncio
import copy
import json
import re
from typing import Callable, Dict, List, Optional, Union
from urllib.parse import parse_qs, unquote, urlencode

import httpx
from httpx import Response
from playwright.async_api import BrowserContext, Page

import config
from tools import utils

from .exception import DataFetchError
from .field import SearchType

import re
import json
from html import unescape
from bs4 import BeautifulSoup

class WeiboClient:
    def __init__(
            self,
            timeout=10,
            proxies=None,
            *,
            headers: Dict[str, str],
            playwright_page: Page,
            cookie_dict: Dict[str, str],
    ):
        self.proxies = proxies
        self.timeout = timeout
        self.headers = headers
        self._host = "https://m.weibo.cn"
        self.playwright_page = playwright_page
        self.cookie_dict = cookie_dict
        self._image_agent_host = "https://i1.wp.com/"

    async def request(self, method, url, **kwargs) -> Union[Response, Dict]:
        enable_return_response = kwargs.pop("return_response", False)
        async with httpx.AsyncClient(proxies=self.proxies) as client:
            response = await client.request(
                method, url, timeout=self.timeout,
                **kwargs
            )

        if enable_return_response:
            return response

        data: Dict = response.json()
        ok_code = data.get("ok")
        if ok_code == 0:  # response error
            utils.logger.error(f"[WeiboClient.request] request {method}:{url} err, res:{data}")
            raise DataFetchError(data.get("msg", "response error"))
        elif ok_code != 1:  # unknown error
            utils.logger.error(f"[WeiboClient.request] request {method}:{url} err, res:{data}")
            raise DataFetchError(data.get("msg", "unknown error"))
        else:  # response right
            return data.get("data", {})

    async def get(self, uri: str, params=None, headers=None, **kwargs) -> Union[Response, Dict]:
        final_uri = uri
        if isinstance(params, dict):
            final_uri = (f"{uri}?"
                         f"{urlencode(params)}")

        if headers is None:
            headers = self.headers
        return await self.request(method="GET", url=f"{self._host}{final_uri}", headers=headers, **kwargs)

    async def post(self, uri: str, data: dict) -> Dict:
        json_str = json.dumps(data, separators=(',', ':'), ensure_ascii=False)
        return await self.request(method="POST", url=f"{self._host}{uri}",
                                  data=json_str, headers=self.headers)

    async def pong(self) -> bool:
        """get a note to check if login state is ok"""
        utils.logger.info("[WeiboClient.pong] Begin pong weibo...")
        ping_flag = False
        try:
            uri = "/api/config"
            resp_data: Dict = await self.request(method="GET", url=f"{self._host}{uri}", headers=self.headers)
            if resp_data.get("login"):
                ping_flag = True
            else:
                utils.logger.error(f"[WeiboClient.pong] cookie may be invalid and again login...")
        except Exception as e:
            utils.logger.error(f"[WeiboClient.pong] Pong weibo failed: {e}, and try to login again...")
            ping_flag = False
        return ping_flag

    async def update_cookies(self, browser_context: BrowserContext):
        cookie_str, cookie_dict = utils.convert_cookies(await browser_context.cookies())
        self.headers["Cookie"] = cookie_str
        self.cookie_dict = cookie_dict

    async def get_note_by_keyword(
            self,
            keyword: str,
            page: int = 1,
            search_type: SearchType = SearchType.DEFAULT
    ) -> Dict:
        """
        search note by keyword
        :param keyword: å¾®åšæœæœçš„å…³é”®è¯
        :param page: åˆ†é¡µå‚æ•° -å½“å‰é¡µç 
        :param search_type: æœç´¢çš„ç±»åž‹ï¼Œè§ weibo/filed.py ä¸­çš„æžšä¸¾SearchType
        :return:
        """
        uri = "/api/container/getIndex"
        containerid = f"100103type={search_type.value}&q={keyword}"
        params = {
            "containerid": containerid,
            "page_type": "searchall",
            "page": page,
        }
        return await self.get(uri, params)

    async def get_note_comments(self, mid_id: str, max_id: int, max_id_type: int = 0) -> Dict:
        """get notes comments
        :param mid_id: å¾®åšID
        :param max_id: åˆ†é¡µå‚æ•°ID
        :param max_id_type: åˆ†é¡µå‚æ•°IDç±»åž‹
        :return:
        """
        uri = "/comments/hotflow"
        params = {
            "id": mid_id,
            "mid": mid_id,
            "max_id_type": max_id_type,
        }
        if max_id > 0:
            params.update({"max_id": max_id})
        referer_url = f"https://m.weibo.cn/detail/{mid_id}"
        headers = copy.copy(self.headers)
        headers["Referer"] = referer_url

        return await self.get(uri, params, headers=headers)

    async def get_note_all_comments(
        self,
        note_id: str,
        crawl_interval: float = 1.0,
        callback: Optional[Callable] = None,
        max_count: int = 10,
    ):
        """
        get note all comments include sub comments
        :param note_id:
        :param crawl_interval:
        :param callback:
        :param max_count:
        :return:
        """
        result = []
        is_end = False
        max_id = -1
        max_id_type = 0
        while not is_end and len(result) < max_count:
            comments_res = await self.get_note_comments(note_id, max_id, max_id_type)
            max_id: int = comments_res.get("max_id")
            max_id_type: int = comments_res.get("max_id_type")
            comment_list: List[Dict] = comments_res.get("data", [])
            is_end = max_id == 0
            if len(result) + len(comment_list) > max_count:
                comment_list = comment_list[:max_count - len(result)]
            if callback:  # å¦‚æžœæœ‰å›žè°ƒå‡½æ•°ï¼Œå°±æ‰§è¡Œå›žè°ƒå‡½æ•°
                await callback(note_id, comment_list)
            await asyncio.sleep(crawl_interval)
            result.extend(comment_list)
            sub_comment_result = await self.get_comments_all_sub_comments(note_id, comment_list, callback)
            result.extend(sub_comment_result)
        return result

    @staticmethod
    async def get_comments_all_sub_comments(note_id: str, comment_list: List[Dict],
                                            callback: Optional[Callable] = None) -> List[Dict]:
        """
        èŽ·å–è¯„è®ºçš„æ‰€æœ‰å­è¯„è®º
        Args:
            note_id:
            comment_list:
            callback:

        Returns:

        """
        if not config.ENABLE_GET_SUB_COMMENTS:
            utils.logger.info(
                f"[WeiboClient.get_comments_all_sub_comments] Crawling sub_comment mode is not enabled")
            return []

        res_sub_comments = []
        for comment in comment_list:
            sub_comments = comment.get("comments")
            if sub_comments and isinstance(sub_comments, list):
                await callback(note_id, sub_comments)
                res_sub_comments.extend(sub_comments)
        return res_sub_comments

    async def get_note_info_by_id(self, note_id: str) -> Dict:
        """
        æ ¹æ®å¸–å­IDèŽ·å–è¯¦æƒ…
        :param note_id:
        :return:
        """
        url = f"{self._host}/detail/{note_id}"
        async with httpx.AsyncClient(proxies=self.proxies) as client:
            response = await client.request(
                "GET", url, timeout=self.timeout, headers=self.headers
            )
            if response.status_code != 200:
                raise DataFetchError(f"get weibo detail err: {response.text}, status code: {response.status_code}")
            match = re.search(r'var \$render_data = (\[.*?\])\[0\]', response.text, re.DOTALL)
            if match:
                render_data_json = match.group(1)
                render_data_dict = json.loads(render_data_json)
                note_detail = render_data_dict[0].get("status")
                note_item = {
                    "mblog": note_detail
                }
                return note_item
            else:
                utils.logger.info(f"[WeiboClient.get_note_info_by_id] æœªæ‰¾åˆ°$render_dataçš„å€¼")
                return dict()

            

            # # ç²¾ç¡®æå–å¾®åšæ­£æ–‡ JSON å­—ç¬¦ä¸²
            # match = re.search(r'"text"\s*:\s*"((?:\\.|[^"\\])*)"', response.text)
            # if match:
            #     raw_html_text = match.group(1)

            #     # æ­£ç¡®è§£ç ï¼šå…ˆåè½¬ä¹‰ï¼Œå† unescape HTML
            #     decoded_html = json.loads(f'"{raw_html_text}"')  # ðŸ”¥ æ­£ç¡®å¤„ç†æ‰€æœ‰ \u å’Œè½¬ä¹‰å­—ç¬¦
            #     decoded_html = unescape(decoded_html)            # HTML ç¬¦å·ï¼Œå¦‚ &gt;

            #     soup = BeautifulSoup(decoded_html, 'html.parser')
            #     plain_text = soup.get_text(separator="\n")
                
            #     dict_temp = {
            #         "mblog": {
            #             "id": note_id,
            #             "text": plain_text,
            #             "created_at": response.headers.get("Date"),
            #             "attitudes_count": 0,
            #             "comments_count": 0,
            #             "reposts_count": 0,
            #         }
            #     }

            #     note_item = {
            #             "mblog": dict_temp
            #         }
            #     return note_item
            
            # else:
            #     utils.logger.info(f"[WeiboClient.get_note_info_by_id] æœªæ‰¾åˆ°$render_dataçš„å€¼")
            #     return dict()

    async def get_note_image(self, image_url: str) -> bytes:
        image_url = image_url[8:]  # åŽ»æŽ‰ https://
        sub_url = image_url.split("/")
        image_url = ""
        for i in range(len(sub_url)):
            if i == 1:
                image_url += "large/"  # éƒ½èŽ·å–é«˜æ¸…å¤§å›¾
            elif i == len(sub_url) - 1:
                image_url += sub_url[i]
            else:
                image_url += sub_url[i] + "/"
        # å¾®åšå›¾åºŠå¯¹å¤–å­˜åœ¨é˜²ç›—é“¾ï¼Œæ‰€ä»¥éœ€è¦ä»£ç†è®¿é—®
        # ç”±äºŽå¾®åšå›¾ç‰‡æ˜¯é€šè¿‡ i1.wp.com æ¥è®¿é—®çš„ï¼Œæ‰€ä»¥éœ€è¦æ‹¼æŽ¥ä¸€ä¸‹
        final_uri = (f"{self._image_agent_host}" f"{image_url}")
        async with httpx.AsyncClient(proxies=self.proxies) as client:
            response = await client.request("GET", final_uri, timeout=self.timeout)
            if not response.reason_phrase == "OK":
                utils.logger.error(f"[WeiboClient.get_note_image] request {final_uri} err, res:{response.text}")
                return None
            else:
                return response.content



    async def get_creator_container_info(self, creator_id: str) -> Dict:
        """
        èŽ·å–ç”¨æˆ·çš„å®¹å™¨ID, å®¹å™¨ä¿¡æ¯ä»£è¡¨ç€çœŸå®žè¯·æ±‚çš„APIè·¯å¾„
            fid_container_idï¼šç”¨æˆ·çš„å¾®åšè¯¦æƒ…APIçš„å®¹å™¨ID
            lfid_container_idï¼šç”¨æˆ·çš„å¾®åšåˆ—è¡¨APIçš„å®¹å™¨ID
        Args:
            creator_id:

        Returns: {

        """
        response = await self.get(f"/u/{creator_id}", return_response=True)
        m_weibocn_params = response.cookies.get("M_WEIBOCN_PARAMS")
        if not m_weibocn_params:
            raise DataFetchError("get containerid failed")
        m_weibocn_params_dict = parse_qs(unquote(m_weibocn_params))
        return {
            "fid_container_id": m_weibocn_params_dict.get("fid", [""])[0],
            "lfid_container_id": m_weibocn_params_dict.get("lfid", [""])[0]
        }

    async def get_creator_info_by_id(self, creator_id: str) -> Dict:
        """
        æ ¹æ®ç”¨æˆ·IDèŽ·å–ç”¨æˆ·è¯¦æƒ…
        Args:
            creator_id:

        Returns:

        """
        uri = "/api/container/getIndex"
        container_info = await self.get_creator_container_info(creator_id)
        if container_info.get("fid_container_id") == "" or container_info.get("lfid_container_id") == "":
            utils.logger.error(f"[WeiboClient.get_creator_info_by_id] get containerid failed")
            raise DataFetchError("get containerid failed")
        params = {
            "jumpfrom": "weibocom",
            "type": "uid",
            "value": creator_id,
            "containerid": container_info["fid_container_id"],
        }

        user_res = await self.get(uri, params)

        if user_res.get("tabsInfo"):
            tabs: List[Dict] = user_res.get("tabsInfo", {}).get("tabs", [])
            for tab in tabs:
                if tab.get("tabKey") == "weibo":
                    container_info["lfid_container_id"] = tab.get("containerid")
                    break

        user_res.update(container_info)
        return user_res

    async def get_notes_by_creator(self, creator: str, container_id: str, since_id: str = "0", ) -> Dict:
        """
        èŽ·å–åšä¸»çš„ç¬”è®°
        Args:
            creator: åšä¸»ID
            container_id: å®¹å™¨ID
            since_id: ä¸Šä¸€é¡µæœ€åŽä¸€æ¡ç¬”è®°çš„ID
        Returns:

        """

        uri = "/api/container/getIndex"
        params = {
            "jumpfrom": "weibocom",
            "type": "uid",
            "value": creator,
            "containerid": container_id,
            "since_id": since_id,
        }
        return await self.get(uri, params)

    async def get_all_notes_by_creator_id(self, creator_id: str, container_id: str, crawl_interval: float = 1.0,
                                          callback: Optional[Callable] = None) -> List[Dict]:
        """
        èŽ·å–æŒ‡å®šç”¨æˆ·ä¸‹çš„æ‰€æœ‰å‘è¿‡çš„å¸–å­ï¼Œè¯¥æ–¹æ³•ä¼šä¸€ç›´æŸ¥æ‰¾ä¸€ä¸ªç”¨æˆ·ä¸‹çš„æ‰€æœ‰å¸–å­ä¿¡æ¯
        Args:
            creator_id:
            container_id:
            crawl_interval:
            callback:

        Returns:

        """
        result = []
        notes_has_more = True
        since_id = ""
        crawler_total_count = 0
        while notes_has_more:
            notes_res = await self.get_notes_by_creator(creator_id, container_id, since_id)
            if not notes_res:
                utils.logger.error(
                    f"[WeiboClient.get_notes_by_creator] The current creator may have been banned by xhs, so they cannot access the data.")
                break
            since_id = notes_res.get("cardlistInfo", {}).get("since_id", "0")
            if "cards" not in notes_res:
                utils.logger.info(
                    f"[WeiboClient.get_all_notes_by_creator] No 'notes' key found in response: {notes_res}")
                break

            notes = notes_res["cards"]
            utils.logger.info(
                f"[WeiboClient.get_all_notes_by_creator] got user_id:{creator_id} notes len : {len(notes)}")
            notes = [note for note  in notes if note.get("card_type") == 9]
            if callback:
                await callback(notes)
            await asyncio.sleep(crawl_interval)
            result.extend(notes)
            crawler_total_count += 10
            notes_has_more = notes_res.get("cardlistInfo", {}).get("total", 0) > crawler_total_count
        return result

